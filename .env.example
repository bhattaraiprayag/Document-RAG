# ================================
# MODEL PROVIDER CONFIGURATION
# ================================

# Provider: "ollama" or "openai"
DEFAULT_PROVIDER=ollama

# Model name based on provider:
# - For Ollama: "qwen3:30b-a3b", "llama3:70b", etc.
# - For OpenAI: "gpt-4-turbo", "gpt-4o", "gpt-3.5-turbo", etc.
DEFAULT_MODEL=qwen3-vl:30b-a3b-instruct

# ================================
# OPENAI CONFIGURATION (if using OpenAI)
# ================================
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1

# ================================
# OLLAMA CONFIGURATION (if using Ollama)
# ================================
# LAN IP of Mac M4 Pro Max running Ollama
OLLAMA_BASE_URL=http://192.168.1.19:11434

# ================================
# SERVICE URLS
# ================================
QDRANT_URL=http://localhost:6333
EMBED_API_URL=http://localhost:8001
RERANK_API_URL=http://localhost:8002

# ================================
# HUGGINGFACE CONFIGURATION
# ================================
# Directory for model cache (relative to project root)
HF_HOME=./models_cache

# ================================
# EMBEDDING API CONFIGURATION
# ================================
# Maximum batch size for embedding requests
MAX_BATCH_SIZE=32
# Batch timeout in seconds
BATCH_TIMEOUT=0.01

# ================================
# RERANKING API CONFIGURATION
# ================================
# Maximum batch size for reranking requests (adjust based on VRAM)
RERANK_BATCH_SIZE=16
# Batch timeout in seconds
RERANK_BATCH_TIMEOUT=0.02

# ================================
# OPTIONAL: LANGFUSE OBSERVABILITY
# ================================
LANGFUSE_HOST=http://localhost:3001
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
